{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibituyi/Encryptix/blob/main/Solar_Flare_Data_Extraction_(2001_2024).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC4BeCmKAssg",
        "outputId": "2c090407-accb-427b-c545-c1eb29fcdc4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4fvXCxYGSJV",
        "outputId": "c269862b-6ca0-494c-e19e-aeb2427e38ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 15/15 [00:05<00:00,  2.85it/s]\n",
            "WARNING:root:Output file /content/drive/MyDrive/Colab Notebooks/combined_solar_flare_data.csv already exists. Overwriting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Timestamp Start Time End Time Peak Time  Location Classification  \\\n",
            "0 2001-01-01       0007     0020      0013                        C   \n",
            "1 2001-01-01       0118     0127      0121                        C   \n",
            "2 2001-01-01       0731     0739      0735  S09E13SF              C   \n",
            "3 2001-01-01       0933     1009      0951  S11W46SF              C   \n",
            "4 2001-01-01       1252     1301      1257                        C   \n",
            "\n",
            "   Intensity  X-ray Flux  Region Number  \n",
            "0       12.0     0.00081            NaN  \n",
            "1       10.0     0.00045            NaN  \n",
            "2       12.0     0.00043         9289.0  \n",
            "3       10.0     0.00210         9283.0  \n",
            "4       10.0     0.00049            NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm  # For progress tracking\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')\n",
        "\n",
        "# Function to clean non-printable characters\n",
        "def clean_row(row):\n",
        "    return re.sub(r'[^\\x20-\\x7E]', '', row)\n",
        "\n",
        "# List of valid classifications\n",
        "VALID_CLASSIFICATIONS = {'A', 'B', 'C', 'M', 'X'}\n",
        "\n",
        "# List of file paths to process\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2001.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2002.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2003.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2004_modified.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2005_modified.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2006_modified.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2007_modified.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2008.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2009.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2010.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2011.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2012.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2013.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2014.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/goes-xrs-report_2015.txt'\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Process each file\n",
        "for file_path in tqdm(file_paths, desc=\"Processing files\"):\n",
        "    try:\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "\n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            logging.error(f\"File not found at {file_path}\")\n",
        "            continue\n",
        "\n",
        "        # Read the file\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = file.read()\n",
        "\n",
        "        # Split data into rows\n",
        "        rows = data.strip().split('\\n')\n",
        "        parsed_data = []\n",
        "\n",
        "        # Parse each row\n",
        "        for row in rows:\n",
        "            try:\n",
        "                # Clean the row\n",
        "                row = clean_row(row)\n",
        "\n",
        "                # Skip empty rows\n",
        "                if not row.strip():\n",
        "                    continue\n",
        "\n",
        "                # Split the row into components\n",
        "                parts = row.split()\n",
        "\n",
        "                # Debug: Log the row and its parts\n",
        "                logging.debug(f\"Row: {row}\")\n",
        "                logging.debug(f\"Parts: {parts}\")\n",
        "\n",
        "                # Extract fields\n",
        "                timestamp = parts[0]\n",
        "                start_time = parts[1]\n",
        "                end_time = parts[2]\n",
        "                peak_time = parts[3]\n",
        "                location = parts[4] if len(parts) > 4 and len(parts[4]) > 1 else ''\n",
        "\n",
        "                # Initialize classification and intensity\n",
        "                classification = np.nan\n",
        "                intensity = np.nan\n",
        "\n",
        "                # Iterate through all parts to find classification and intensity\n",
        "                classification_index = -1\n",
        "                for i, part in enumerate(parts):\n",
        "                    # Check if the part is a valid classification\n",
        "                    if part.upper() in VALID_CLASSIFICATIONS:\n",
        "                        classification = part.upper()  # Found classification\n",
        "                        classification_index = i  # Record the index of the classification\n",
        "                        # Check if the next part is the intensity\n",
        "                        if i + 1 < len(parts) and parts[i + 1].replace('.', '').isdigit():\n",
        "                            intensity = parts[i + 1]  # Intensity is next field\n",
        "                        break\n",
        "\n",
        "                # Debug: Log the extracted classification and intensity\n",
        "                logging.debug(f\"Classification: {classification}, Intensity: {intensity}\")\n",
        "\n",
        "                # Handle X-ray flux and region number\n",
        "                xray_flux = np.nan\n",
        "                region_number = np.nan\n",
        "\n",
        "                # Check if X-ray flux is in the correct column\n",
        "                if len(parts) > 7 and re.match(r'^-?\\d*\\.?\\d+([Ee][-+]?\\d+)?$', parts[7]):\n",
        "                    xray_flux = parts[7]  # X-ray flux is present\n",
        "                elif len(parts) > 8 and re.match(r'^-?\\d*\\.?\\d+([Ee][-+]?\\d+)?$', parts[8]):\n",
        "                    xray_flux = parts[8]  # X-ray flux is in the region number column\n",
        "                    region_number = np.nan  # Region number is missing\n",
        "                else:\n",
        "                    xray_flux = np.nan  # X-ray flux is missing\n",
        "\n",
        "                # Handle region number by finding the first numerical value from the end\n",
        "                # Only consider parts after the classification column\n",
        "                if classification_index != -1:\n",
        "                    for part in reversed(parts[classification_index + 2:]):  # Skip classification and intensity\n",
        "                        if part.replace('.', '').isdigit() and part != intensity:\n",
        "                            region_number = part\n",
        "                            break\n",
        "\n",
        "                # Fix the timestamp format\n",
        "                # Remove the first 5 characters (code) and parse the remaining as 'YYMMDD'\n",
        "                if len(timestamp) >= 11:  # Ensure the timestamp is at least 11 characters long\n",
        "                    timestamp = timestamp[5:11]  # Extract the next 6 characters as 'YYMMDD'\n",
        "                    try:\n",
        "                        # Parse the date and add a default time ('00:00:00')\n",
        "                        timestamp = pd.to_datetime(timestamp, format='%y%m%d', errors='coerce')\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error parsing timestamp {timestamp}: {e}\")\n",
        "                        continue\n",
        "                else:\n",
        "                    logging.warning(f\"Skipping row with invalid timestamp length: {timestamp}\")\n",
        "                    continue\n",
        "\n",
        "                # Append valid row to parsed_data\n",
        "                parsed_data.append([\n",
        "                    timestamp, start_time, end_time, peak_time, location,\n",
        "                    classification, intensity, xray_flux, region_number\n",
        "                ])\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error parsing row: {row}. Error: {e}\")\n",
        "\n",
        "        # Create DataFrame for the current file\n",
        "        columns = [\n",
        "            'Timestamp', 'Start Time', 'End Time', 'Peak Time', 'Location',\n",
        "            'Classification', 'Intensity', 'X-ray Flux', 'Region Number'\n",
        "        ]\n",
        "        df = pd.DataFrame(parsed_data, columns=columns)\n",
        "\n",
        "        # Clean the 'Classification' column\n",
        "        df['Classification'] = df['Classification'].replace('', np.nan)\n",
        "\n",
        "        # Clean the 'Intensity' column\n",
        "        df['Intensity'] = df['Intensity'].replace('', np.nan)\n",
        "        df['Intensity'] = pd.to_numeric(df['Intensity'], errors='coerce')\n",
        "\n",
        "        # Clean the 'X-ray Flux' column\n",
        "        df['X-ray Flux'] = df['X-ray Flux'].replace('', np.nan).astype(float)\n",
        "\n",
        "        # Clean the 'Region Number' column\n",
        "        df['Region Number'] = df['Region Number'].replace('', np.nan)\n",
        "        df['Region Number'] = pd.to_numeric(df['Region Number'], errors='coerce')\n",
        "\n",
        "        # Append the DataFrame to the list\n",
        "        dataframes.append(df)\n",
        "        logging.info(f\"Successfully processed {len(df)} rows from {file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "# Combine all DataFrames into one\n",
        "if dataframes:\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "    logging.info(f\"Combined DataFrame contains {len(combined_df)} rows.\")\n",
        "\n",
        "    # Display the combined DataFrame\n",
        "    logging.info(\"\\nCombined DataFrame:\")\n",
        "    print(combined_df.head())\n",
        "\n",
        "    # Save the combined DataFrame to a CSV file (optional)\n",
        "    output_file = '/content/drive/MyDrive/Colab Notebooks/combined_solar_flare_data.csv'\n",
        "    if os.path.exists(output_file):\n",
        "        logging.warning(f\"Output file {output_file} already exists. Overwriting...\")\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "    logging.info(f\"Combined data saved to {output_file}\")\n",
        "else:\n",
        "    logging.warning(\"No data was processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_RVIH0D5aG6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMvU4bTST3MKGmF2mPzp8qe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}